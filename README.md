# Adobe-hackathon-round1a
Adobe India Hackathon - Round 1A 

This project provides a solution for Round 1A of the Adobe India Hackathon 2025, focusing on extracting a structured outline (Title, H1, H2, H3 with page numbers) from PDF documents and outputting it in a specified JSON format. The solution is designed to meet all the stringent performance and operational constraints, including CPU-only execution, limited model size, and offline capability.

### Approach
Our approach to accurately extract document outlines from raw PDFs combines robust PDF parsing with a lightweight, custom-trained Machine Learning model, complemented by intelligent post-processing heuristics.

**Granular PDF Parsing:** We utilize pdfminer.six to parse PDF documents. Crucially, we extract individual text lines (LTTextBoxHorizontal elements) along with their detailed properties, such as:

- Font Size: Absolute and relative to the page's median font size.

- Font Style: Detection of bold and italic attributes, and a numerical font weight score.

- Positioning: X-coordinate (horizontal alignment) and vertical spacing from the previous text element.

- Text Characteristics: Line length, character density, presence of numerical/alphabetical prefixes (e.g., "1.", "Chapter"), and a flag for purely numeric lines (e.g., page numbers).

- Page Number: The 0-indexed page number where the text appears.

**Machine Learning Classification:**

- A Logistic Regression model (a simple, fast, and small classifier from scikit-learn) is trained to classify each text line as either H1, H2, H3, or Body text.

- The model is trained on a dataset generated by leveraging a powerful Large Language Model (LLM) like Gemini. The LLM processes a diverse collection of PDFs and generates high-quality, structured JSON outlines. This LLM-generated data serves as our "ground truth," allowing our local model to learn complex patterns without requiring manual annotation or large, pre-trained models at runtime.

- Class Imbalance Handling: During training, ``class_weight='balanced'`` is used to ensure the model pays adequate attention to minority classes (H1, H2, H3), preventing it from simply predicting "Body" for everything.

**Intelligent Post-processing & Filtering:** After the ML model makes its predictions, a series of heuristic rules are applied to refine the output and remove noise:

- Multi-line Heading Correction: If a predicted heading contains newline characters, only the first line is retained, preventing body text from being included in a heading.

- Noise Filtering: Aggressive filters remove common non-heading elements such as page numbers, version indicators, and repetitive headers/footers (e.g., "Page X of Y", "Revision History").

- De-duplication: Ensures that identical headings are not listed multiple times consecutively.

- Title Extraction: The main document title is identified heuristically, typically as the largest, most prominent text on the first page that is not a common header or page number.

This hybrid approach ensures high accuracy by learning from rich features and LLM-generated ground truth, while strictly adhering to the hackathon's constraints by using a lightweight, locally-trained model during execution.

### Models and Libraries Used:
- ``pdfminer.six``: For robust PDF text and layout extraction.

- ``numpy``: For numerical operations, especially in feature extraction (e.g., median font size calculation).

- ``scikit-learn``: For the Machine Learning model (``LogisticRegression`` classifier) and data preprocessing (StandardScaler).

- ``joblib``: For efficient saving and loading of the trained scikit-learn model and scaler.

- Custom-trained Logistic Regression Model: A lightweight, locally trained model (not a large, external LLM) is used for inference, ensuring compliance with the size and offline constraints.

### How to Build & Run:

**Prerequisite:**  
- Docker installed and running on your system (Windows, macOS, or Linux).
- Follow the project directory structure as in the repo.
- The model/ directory must contain the ``outline_classifier.joblib`` and ``scaler.joblib`` files. These are generated by running the ``train_model.ipynb`` in Google Colab (which leverages an LLM to create the training data, then trains the local model).

**Build Command (as specified in the problem statement):**

Navigate to your adobe-hackathon-round1a directory in your terminal/PowerShell and run:

`docker build --platform linux/amd64 -t mysolutionname:somerandomidentifier .`

- Replace mysolutionname:somerandomidentifier with your chosen image name and tag (e.g., adobe-outline-extractor:v1.0).

- It's recommended to use --no-cache flag during development to ensure all changes are picked up:

`docker build --no-cache --platform linux/amd64 -t adobe-outline-extractor:v1.0 .`

**Run Command (as specified in the problem statement):**

After building the image, run the solution using:

`docker run --rm -v "$(pwd)/input:/app/input" -v "$(pwd)/output:/app/output" --network none mysolutionname:somerandomidentifier`

Replace mysolutionname:somerandomidentifier with the exact image name and tag used during the build step.

- ``--rm``: Automatically removes the container after it exits.

- ``-v "$(pwd)/input:/app/input"``: Mounts your local input directory to /app/input inside the container.

- ``-v "$(pwd)/output:/app/output"``: Mounts your local output directory to /app/output inside the container.

- ``--network none``: Ensures no network access during execution, fulfilling the offline constraint.

The container will automatically process all PDF files found in your local input/ directory and generate corresponding .json outline files in your local output/ directory.
