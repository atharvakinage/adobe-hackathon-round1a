{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install specific versions of libraries to match your Docker environment\n",
        "# This is crucial to avoid version incompatibility issues when loading models.\n",
        "!pip install scikit-learn==1.3.0 joblib==1.3.2 numpy==1.25.2 pdfminer.six==20221105\n",
        "\n",
        "# IMPORTANT: After running the above pip install command,\n",
        "# YOU MUST RESTART THE COLAB RUNTIME before proceeding.\n",
        "# Go to \"Runtime\" -> \"Restart runtime\" in the Colab menu.\n",
        "# Then, run all cells from the beginning again.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "from pdfminer.high_level import extract_pages\n",
        "from pdfminer.layout import LTTextContainer, LTChar, LTLine, LTRect, LTFigure, LTTextBoxHorizontal # Import LTTextBoxHorizontal\n",
        "\n",
        "# --- Configuration for GitHub Repository and Model Saving ---\n",
        "GITHUB_REPO_URL = \"https://github.com/jhaaj08/Adobe-India-Hackathon25.git\"\n",
        "REPO_NAME = GITHUB_REPO_URL.split('/')[-1].replace('.git', '')\n",
        "CHALLENGE_1A_DIR = os.path.join(REPO_NAME, \"Challenge_1a\")\n",
        "SAMPLE_INPUT_DIR = os.path.join(CHALLENGE_1A_DIR, \"sample_dataset/pdfs\")\n",
        "SAMPLE_OUTPUT_DIR = os.path.join(CHALLENGE_1A_DIR, \"sample_dataset/outputs\")\n",
        "\n",
        "MODEL_DIR = \"colab_model_files\"\n",
        "MODEL_PATH = os.path.join(MODEL_DIR, \"outline_classifier.joblib\")\n",
        "SCALER_PATH = os.path.join(MODEL_DIR, \"scaler.joblib\")\n",
        "\n",
        "# --- Feature Extraction Functions (Copied and adapted from outline_extractor.py) ---\n",
        "def get_font_weight_score(fontname):\n",
        "    \"\"\"Assigns a numerical score based on font name to indicate boldness.\"\"\"\n",
        "    if not fontname:\n",
        "        return 0\n",
        "    fontname_lower = fontname.lower()\n",
        "    score = 0\n",
        "    if 'bold' in fontname_lower or 'bd' in fontname_lower or 'heavy' in fontname_lower or 'black' in fontname_lower:\n",
        "        score += 2\n",
        "    if 'italic' in fontname_lower or 'it' in fontname_lower:\n",
        "        score += 0.5\n",
        "    return score\n",
        "\n",
        "def get_text_properties(element, page_width, page_height, page_font_sizes=None, prev_element_bbox=None):\n",
        "    \"\"\"\n",
        "    Extracts a rich set of features from a PDF text element (LTTextBoxHorizontal).\n",
        "    Requires page_width and page_height for normalization.\n",
        "    \"\"\"\n",
        "    if not isinstance(element, LTTextBoxHorizontal): # Now expecting LTTextBoxHorizontal\n",
        "        return None\n",
        "\n",
        "    text = element.get_text().strip()\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    font_size = 0\n",
        "    font_is_bold = False\n",
        "    font_is_italic = False\n",
        "    font_weight_score = 0\n",
        "\n",
        "    # Iterate through text lines and characters within the LTTextBoxHorizontal\n",
        "    # to get font properties. Assume consistent font within a single line.\n",
        "    for text_line in element:\n",
        "        for character in text_line:\n",
        "            if isinstance(character, LTChar):\n",
        "                font_size = round(character.size, 2)\n",
        "                fontname = character.fontname.lower()\n",
        "                font_weight_score = get_font_weight_score(character.fontname)\n",
        "                if 'bold' in fontname or 'bd' in fontname:\n",
        "                    font_is_bold = True\n",
        "                if 'italic' in fontname or 'it' in fontname:\n",
        "                    font_is_italic = True\n",
        "                break # Found first character, assume consistent for the line\n",
        "        if font_size > 0: # If font size found, no need to check other lines in this box\n",
        "            break\n",
        "\n",
        "    relative_font_size = 0\n",
        "    if page_font_sizes and font_size:\n",
        "        median_font_size = np.median(list(page_font_sizes))\n",
        "        if median_font_size > 0:\n",
        "            relative_font_size = font_size / median_font_size\n",
        "\n",
        "    vertical_space_above = 0\n",
        "    if prev_element_bbox and element.bbox:\n",
        "        vertical_space_above = element.bbox[1] - prev_element_bbox[3]\n",
        "        vertical_space_above = max(0, min(vertical_space_above, 100))\n",
        "\n",
        "    x_position_normalized = element.bbox[0] / page_width if page_width > 0 else 0\n",
        "\n",
        "    line_width = element.bbox[2] - element.bbox[0]\n",
        "    char_density = len(text) / line_width if line_width > 0 else 0\n",
        "\n",
        "    has_prefix = 0\n",
        "    if text and len(text) > 2:\n",
        "        first_word = text.split(' ')[0]\n",
        "        # More robust prefix check: \"1.\", \"1.1.\", \"A.\", \"Chapter X\", \"Section Y\"\n",
        "        if (first_word.endswith('.') and (first_word[:-1].isdigit() or first_word[:-1].isalpha())) or \\\n",
        "           (first_word.isdigit() and len(first_word) < 4) or \\\n",
        "           (first_word.isupper() and len(first_word) < 8 and first_word.isalpha()) or \\\n",
        "           (text.lower().startswith(\"chapter \") and len(text.split()) < 5) or \\\n",
        "           (text.lower().startswith(\"section \") and len(text.split()) < 5):\n",
        "            has_prefix = 1\n",
        "\n",
        "    is_numeric_only = text.replace('.', '').replace(',', '').replace(' ', '').isdigit() and len(text) < 10\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"font_size\": font_size,\n",
        "        \"is_uppercase\": text.isupper(),\n",
        "        \"is_bold\": font_is_bold,\n",
        "        \"is_italic\": font_is_italic,\n",
        "        \"font_weight_score\": font_weight_score,\n",
        "        \"line_length\": len(text),\n",
        "        \"x_position\": element.bbox[0],\n",
        "        \"x_position_normalized\": x_position_normalized,\n",
        "        \"relative_font_size\": relative_font_size,\n",
        "        \"vertical_space_above\": vertical_space_above,\n",
        "        \"has_prefix\": has_prefix,\n",
        "        \"char_density\": char_density,\n",
        "        \"is_numeric_only\": is_numeric_only,\n",
        "        \"bbox\": element.bbox\n",
        "    }\n",
        "\n",
        "def normalize_text_for_comparison(text):\n",
        "    \"\"\"Normalizes text for robust comparison with ground truth.\"\"\"\n",
        "    return ' '.join(text.lower().strip().split()).replace('\\n', ' ')\n",
        "\n",
        "def collect_real_training_data():\n",
        "    \"\"\"\n",
        "    Collects features and labels from the sample input/output PDFs.\n",
        "    \"\"\"\n",
        "    X = [] # Features\n",
        "    y = [] # Labels\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(SAMPLE_INPUT_DIR) if f.lower().endswith(\".pdf\")]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in {SAMPLE_INPUT_DIR}. Please check the repository structure.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    for pdf_filename in pdf_files:\n",
        "        pdf_path = os.path.join(SAMPLE_INPUT_DIR, pdf_filename)\n",
        "        json_filename = os.path.splitext(pdf_filename)[0] + \".json\"\n",
        "        json_path = os.path.join(SAMPLE_OUTPUT_DIR, json_filename)\n",
        "\n",
        "        if not os.path.exists(json_path):\n",
        "            print(f\"Warning: No corresponding JSON found for {pdf_filename} at {json_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing training data for: {pdf_filename}\")\n",
        "\n",
        "        # Load ground truth JSON\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            ground_truth = json.load(f)\n",
        "\n",
        "        ground_truth_outline_map = {}\n",
        "        for entry in ground_truth.get('outline', []):\n",
        "            normalized_gt_text = normalize_text_for_comparison(entry['text'])\n",
        "            # Store a mapping of normalized text to its level\n",
        "            ground_truth_outline_map[normalized_gt_text] = entry['level']\n",
        "\n",
        "        all_elements_raw = []\n",
        "        for page_num, page_layout in enumerate(extract_pages(pdf_path)):\n",
        "            page_width = page_layout.bbox[2] - page_layout.bbox[0]\n",
        "            page_height = page_layout.bbox[3] - page_layout.bbox[1]\n",
        "\n",
        "            page_elements_on_page = []\n",
        "            for element in page_layout:\n",
        "                # Process LTTextBoxHorizontal for individual lines\n",
        "                if isinstance(element, LTTextBoxHorizontal): # Now expecting LTTextBoxHorizontal for training\n",
        "                    page_elements_on_page.append({\n",
        "                        \"element\": element,\n",
        "                        \"page\": page_num + 1,\n",
        "                        \"bbox\": element.bbox,\n",
        "                        \"page_width\": page_width,\n",
        "                        \"page_height\": page_height\n",
        "                    })\n",
        "                elif isinstance(element, (LTLine, LTRect, LTFigure)):\n",
        "                    page_elements_on_page.append({\n",
        "                        \"element\": element,\n",
        "                        \"page\": page_num + 1,\n",
        "                        \"bbox\": element.bbox,\n",
        "                        \"page_width\": page_width,\n",
        "                        \"page_height\": page_height\n",
        "                    })\n",
        "            page_elements_on_page.sort(key=lambda x: x[\"bbox\"][1], reverse=True)\n",
        "            all_elements_raw.extend(page_elements_on_page)\n",
        "\n",
        "        prev_element_bbox = None\n",
        "        page_font_sizes_cache = {}\n",
        "\n",
        "        for elem_data in all_elements_raw:\n",
        "            element = elem_data[\"element\"]\n",
        "            page_num = elem_data[\"page\"]\n",
        "            page_width = elem_data[\"page_width\"]\n",
        "            page_height = elem_data[\"page_height\"]\n",
        "\n",
        "            if isinstance(element, LTTextBoxHorizontal): # Only process text boxes for features\n",
        "                if page_num not in page_font_sizes_cache:\n",
        "                    current_page_text_elements = [\n",
        "                        e[\"element\"] for e in all_elements_raw if e[\"page\"] == page_num and isinstance(e[\"element\"], LTTextBoxHorizontal)\n",
        "                    ]\n",
        "                    page_font_sizes_cache[page_num] = {\n",
        "                        get_text_properties(e, page_width, page_height)[\"font_size\"]\n",
        "                        for e in current_page_text_elements if get_text_properties(e, page_width, page_height) and get_text_properties(e, page_width, page_height)[\"font_size\"] > 0\n",
        "                    }\n",
        "\n",
        "                props = get_text_properties(element, page_width, page_height, page_font_sizes_cache.get(page_num), prev_element_bbox)\n",
        "\n",
        "                if props:\n",
        "                    # Determine label based on ground truth\n",
        "                    normalized_text = normalize_text_for_comparison(props[\"text\"])\n",
        "\n",
        "                    assigned_level = 'Body' # Default to Body\n",
        "                    # Check for direct match first\n",
        "                    if normalized_text in ground_truth_outline_map:\n",
        "                        assigned_level = ground_truth_outline_map[normalized_text]\n",
        "                    else:\n",
        "                        # Fallback to fuzzy matching if direct match fails\n",
        "                        # This helps with minor parsing differences or extra spaces/newlines\n",
        "                        for gt_norm_text, gt_level in ground_truth_outline_map.items():\n",
        "                            if gt_norm_text in normalized_text or normalized_text in gt_norm_text:\n",
        "                                # Prioritize exact or longer matches if multiple fuzzy matches exist\n",
        "                                # For simplicity, first fuzzy match wins for now.\n",
        "                                assigned_level = gt_level\n",
        "                                break\n",
        "\n",
        "                    # Map level to numerical label\n",
        "                    level_to_int = {\"H1\": 0, \"H2\": 1, \"H3\": 2, \"Body\": 3}\n",
        "                    label = level_to_int.get(assigned_level, 3) # Default to Body (3) if not found\n",
        "\n",
        "                    # Append features and label\n",
        "                    X.append([\n",
        "                        props[\"font_size\"],\n",
        "                        int(props[\"is_uppercase\"]),\n",
        "                        int(props[\"is_bold\"]),\n",
        "                        int(props[\"is_italic\"]),\n",
        "                        props[\"font_weight_score\"],\n",
        "                        props[\"line_length\"],\n",
        "                        props[\"x_position\"],\n",
        "                        props[\"x_position_normalized\"],\n",
        "                        props[\"relative_font_size\"],\n",
        "                        props[\"vertical_space_above\"],\n",
        "                        int(props[\"has_prefix\"]),\n",
        "                        props[\"char_density\"],\n",
        "                        int(props[\"is_numeric_only\"])\n",
        "                    ])\n",
        "                    y.append(label)\n",
        "                prev_element_bbox = props[\"bbox\"] if props else element.bbox\n",
        "            else: # Non-text element, just update prev_element_bbox\n",
        "                prev_element_bbox = element.bbox\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def train_and_save_model():\n",
        "    \"\"\"\n",
        "    Trains a Logistic Regression model and a StandardScaler using real sample data, then saves them.\n",
        "    \"\"\"\n",
        "    # Clone the GitHub repository\n",
        "    print(f\"Cloning {GITHUB_REPO_URL}...\")\n",
        "    if os.path.exists(REPO_NAME):\n",
        "        !rm -rf {REPO_NAME} # Clean up if already exists\n",
        "    !git clone {GITHUB_REPO_URL}\n",
        "\n",
        "    print(\"Collecting real training data from sample PDFs and JSONs...\")\n",
        "    X, y = collect_real_training_data()\n",
        "\n",
        "    if X.size == 0:\n",
        "        print(\"No training data collected. Exiting training process.\")\n",
        "        return\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Initialize and train the StandardScaler\n",
        "    print(\"Training StandardScaler...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Initialize and train the Logistic Regression model\n",
        "    print(\"Training Logistic Regression model...\")\n",
        "    model = LogisticRegression(max_iter=2000, random_state=42, solver='lbfgs', multi_class='multinomial')\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    print(\"\\nModel Classification Report:\")\n",
        "    target_names = ['H1', 'H2', 'H3', 'Body']\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "    # Create the model directory if it doesn't exist\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    # Save the trained model and scaler\n",
        "    print(f\"Saving model to {MODEL_PATH}\")\n",
        "    joblib.dump(model, MODEL_PATH)\n",
        "    print(f\"Saving scaler to {SCALER_PATH}\")\n",
        "    joblib.dump(scaler, SCALER_PATH)\n",
        "    print(\"Model and scaler saved successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_save_model()\n",
        "\n",
        "# --- Code to download the files from Colab ---\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n--- Training complete. Downloading model files ---\")\n",
        "# Create a zip archive of the model files for easy download\n",
        "!zip -r /content/colab_model_files.zip {MODEL_DIR}\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/colab_model_files.zip')\n",
        "\n",
        "print(\"Model and scaler files zipped and downloaded to your local machine.\")\n",
        "print(\"Extract 'colab_model_files.zip' and place 'outline_classifier.joblib' and 'scaler.joblib' into your local 'adobe-hackathon-round1a/model/' directory.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.3.0 in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: joblib==1.3.2 in /usr/local/lib/python3.11/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.11/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /usr/local/lib/python3.11/dist-packages (20221105)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20221105) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20221105) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105) (2.22)\n",
            "Cloning https://github.com/jhaaj08/Adobe-India-Hackathon25.git...\n",
            "Cloning into 'Adobe-India-Hackathon25'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 124 (delta 26), reused 52 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (124/124), 19.38 MiB | 23.24 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "Collecting real training data from sample PDFs and JSONs...\n",
            "Processing training data for: file02.pdf\n",
            "Processing training data for: file05.pdf\n",
            "Processing training data for: file01.pdf\n",
            "Processing training data for: file04.pdf\n",
            "Processing training data for: file03.pdf\n",
            "Training StandardScaler...\n",
            "Training Logistic Regression model...\n",
            "\n",
            "Model Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          H1       1.00      0.20      0.33         5\n",
            "          H2       0.80      0.67      0.73         6\n",
            "          H3       0.57      0.67      0.62         6\n",
            "        Body       0.92      0.96      0.94        84\n",
            "\n",
            "    accuracy                           0.89       101\n",
            "   macro avg       0.82      0.62      0.65       101\n",
            "weighted avg       0.90      0.89      0.88       101\n",
            "\n",
            "Saving model to colab_model_files/outline_classifier.joblib\n",
            "Saving scaler to colab_model_files/scaler.joblib\n",
            "Model and scaler saved successfully!\n",
            "\n",
            "--- Training complete. Downloading model files ---\n",
            "updating: colab_model_files/ (stored 0%)\n",
            "updating: colab_model_files/scaler.joblib (deflated 16%)\n",
            "updating: colab_model_files/outline_classifier.joblib (deflated 21%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fdbdcce7-46fc-47de-a55e-31c280ff682c\", \"colab_model_files.zip\", 2395)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and scaler files zipped and downloaded to your local machine.\n",
            "Extract 'colab_model_files.zip' and place 'outline_classifier.joblib' and 'scaler.joblib' into your local 'adobe-hackathon-round1a/model/' directory.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "cr_OA9V7rQkY",
        "outputId": "6627fe47-5ed3-4e8b-d8d9-d4dcc75e77d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}